{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reader(path, first_var):\n",
    "    '''\n",
    "    Reads SEMS/DASH data, adds datetime columns\n",
    "\n",
    "    :param path: path to data file\n",
    "    :param first_var: the name of the first column label\n",
    "    :return: pandas DataFrame\n",
    "    '''\n",
    "    # Open the file and read the lines\n",
    "    skip=1\n",
    "    with open(path, \"r\") as file:\n",
    "        # Iterate over the lines\n",
    "        for line in file:\n",
    "            # Strip leading and trailing whitespace\n",
    "            line = line.strip()\n",
    "            # Check if the line contains column names\n",
    "            if line.startswith(first_var):\n",
    "                # Split the line by whitespace and append to the columns list\n",
    "                columns = line[1:].strip().split(\"\\t\")\n",
    "                break\n",
    "            skip+=1\n",
    "    # Read the data into a DataFrame, skipping the first 6 rows of comments\n",
    "    d = pd.read_csv(path, sep='\\t', skiprows=skip, names=columns)\n",
    "    #Check for duplicated metadata, remove metadata rows based on string \"OPC SN\"\n",
    "    if len(d)>0:\n",
    "        if isinstance(d.iloc[0,0], str):\n",
    "            dup_meta = [n for n, i in enumerate(d.iloc[:,0]) if 'OPC SN' in i]\n",
    "            if len(dup_meta) > 0:\n",
    "                for line in dup_meta:\n",
    "                    #Deletes duplicate metadata rows from dataframe\n",
    "                    d.drop(np.arange(line,line+68), inplace = True)\n",
    "                    d = d.apply(pd.to_numeric, errors='ignore')\n",
    "                    \n",
    "    # Creates datetime columns\n",
    "    if 'DOY.Frac' in d.keys():\n",
    "        d['dt'] = pd.to_datetime('2024-1-1') + pd.to_timedelta(d['DOY.Frac'], unit='D') - pd.Timedelta(days=1)\n",
    "    if 'StartTimeSt' in d.keys():\n",
    "        d['st_dt'] = pd.to_datetime('2024-1-1') + pd.to_timedelta(d['StartTimeSt'], unit='D') - pd.Timedelta(days=1)\n",
    "    if 'EndTimeSt' in d.keys():\n",
    "        d['end_dt'] = pd.to_datetime('2024-1-1') + pd.to_timedelta(d['EndTimeSt'], unit='D') - pd.Timedelta(days=1)\n",
    "    if 'YY/MM/DD' and 'HR:MN:SC' in d.keys():\n",
    "        d['dt'] = pd.to_datetime(str(20) + d['YY/MM/DD'] + ' ' + d['HR:MN:SC'], format='%Y/%m/%d %H:%M:%S')\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glob_reader(file_key, first_var, subfolder = './data/'):\n",
    "    '''\n",
    "    Reads groups of data files and merges them into one\n",
    "\n",
    "    :param file_key: shared key in filenames\n",
    "    :param first_var: the name of the first column label\n",
    "    :param subfolder: name of the subfolder containing the data\n",
    "    :return: pandas DataFrame\n",
    "    '''\n",
    "    paths = sorted(glob.glob(subfolder+'*'+file_key+'*'))\n",
    "    d = []\n",
    "    for i in range(0, len(paths)):\n",
    "        d.append(reader(paths[i], first_var))\n",
    "    d = pd.concat(d).reset_index()\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bin_sum(d):\n",
    "    d = d.copy()\n",
    "    col_w_bin = [col for col in d.columns if 'bin' in col]\n",
    "    d['bin_sum'] = d[col_w_bin].sum(numeric_only = True, axis=1)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dNdlogDp(data, bins):\n",
    "    #Calculating dN/dlogDp for OPCs; requires dlogDp to be calculated for bins\n",
    "    dNdlogDp = []\n",
    "    for binN in bins.index[:-1]:\n",
    "        dNdlogDp.append(data[f'bin{binN}']/bins.loc[binN, 'dlogDp'])\n",
    "    dNdlogDp = pd.concat(dNdlogDp, axis = 1)\n",
    "    dNdlogDp.columns = [f'{i}_norm' for i in dNdlogDp.columns]\n",
    "        #Norm = /dlogDp\n",
    "    return dNdlogDp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = pd.read_csv('./DASH_Bins_2023.csv').set_index('BinNum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read OPC Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "dopc = add_bin_sum(glob_reader('OPC_210', '#YY/MM/DD', subfolder = '../data/DASH_test-fight-240517/'))\n",
    "hopc = add_bin_sum(glob_reader('OPC_212', '#YY/MM/DD', subfolder = '../data/DASH_test-fight-240517/'))\n",
    "dash = glob_reader('DASH_FLOW', '#DOY.Frac', subfolder = '../data/DASH_test-fight-240517/')\n",
    "sems = glob_reader('SEMS_DATA', '#DOY.Frac', subfolder = '../data/DASH_test-fight-240517/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge_asof(dash, sems, on='dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge_asof(merged, dopc, on='dt').drop(columns=['index_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge_asof(merged, hopc, on='dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_diff = merged[['dt', 'UpSt_Dia', 'HO_RH']].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[['dt.diff', 'UpSt_Dia.diff', 'HO_RH.diff']] = merged_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_w_bin = [c for c in merged.columns if 'bin' in c and 'sum' not in c]\n",
    "\n",
    "new_group = True\n",
    "start_i = []\n",
    "end_i = []\n",
    "for i in range(0,len(merged)):\n",
    "    row = merged.iloc[i]\n",
    "    if new_group:\n",
    "        start_i.append(i)\n",
    "        new_group = False\n",
    "    if abs(row['UpSt_Dia.diff']) > 0:\n",
    "        end_i.append(i)\n",
    "        new_group = True\n",
    "    elif abs(row['HO_RH.diff']) > 5:\n",
    "        end_i.append(i)\n",
    "        new_group = True\n",
    "    if i == len(merged)-1:\n",
    "        if len(start_i) > len(end_i):\n",
    "            end_i.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['Time_Start', 'Time_End', 'Dp', 'RH']\n",
    "comb_bins = []\n",
    "dopc_labels = []\n",
    "hopc_labels = []\n",
    "for i in range(1,73):\n",
    "    if i < 10:\n",
    "        dopc_labels.append('D_Bin0'+str(i))\n",
    "        hopc_labels.append('H_Bin0'+str(i))\n",
    "    else:\n",
    "        dopc_labels.append('D_Bin'+str(i))\n",
    "        hopc_labels.append('H_Bin'+str(i))\n",
    "    \n",
    "    if i < 10:\n",
    "        col.append('D_Bin0'+str(i))\n",
    "        comb_bins.append('D_Bin0'+str(i))\n",
    "    else:\n",
    "        col.append('D_Bin'+str(i))\n",
    "        comb_bins.append('D_Bin'+str(i))\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1,73):\n",
    "    if i < 10:\n",
    "        col.append('H_Bin0'+str(i))\n",
    "        comb_bins.append('H_Bin0'+str(i))\n",
    "    else:\n",
    "        col.append('H_Bin'+str(i))\n",
    "        comb_bins.append('H_Bin'+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(columns = col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(columns = col)\n",
    "out_acc = 0\n",
    "for i in range(0,len(start_i)):\n",
    "    subset = merged.iloc[start_i[i]:end_i[i]+1].reset_index()\n",
    "    n_sum = 0\n",
    "    dt_start = subset.loc[0,'dt']\n",
    "    s = 0\n",
    "    for j in range(0,len(subset)):\n",
    "        n_sum += subset.loc[j, 'bin_sum_x']\n",
    "        if subset.loc[j, 'dt.diff'] > timedelta(seconds=2):\n",
    "            break\n",
    "        if subset.loc[j, 'dt'] - dt_start > timedelta(minutes=5):\n",
    "            break\n",
    "        if n_sum > 100:\n",
    "            output.loc[out_acc, 'Time_Start'] = subset.loc[s, 'dt']\n",
    "            output.loc[out_acc, 'Time_End'] = subset.loc[j, 'dt']\n",
    "\n",
    "            output.loc[out_acc, 'Dp'] = subset.loc[s:j+1, 'UpSt_Dia'].mean()\n",
    "            output.loc[out_acc, 'RH'] = subset.loc[s:j+1, 'HO_RH'].mean()\n",
    "\n",
    "            output.loc[out_acc, 'Dp_std'] = subset.loc[s:j+1, 'UpSt_Dia'].std()\n",
    "            output.loc[out_acc, 'RH_std'] = subset.loc[s:j+1, 'HO_RH'].std()\n",
    "\n",
    "            output.loc[out_acc, comb_bins] = list(subset.loc[s:j+1, col_w_bin].sum())\n",
    "            \n",
    "            s = j\n",
    "            dt_start = subset.loc[s,'dt']\n",
    "            n_sum = 0\n",
    "            out_acc += 1\n",
    "\n",
    "output = output[output['RH_std']<5]\n",
    "output = output[output['Dp_std']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timedelta('0 days 00:00:25.884985263')"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(output['Time_End'] - output['Time_Start']).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
