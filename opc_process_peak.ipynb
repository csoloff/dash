{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPC Resampling Script\n",
    "\n",
    "This script sums rows of the OPC size distributions reaches a threshold peak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "import datetime as dt\n",
    "from datetime import time\n",
    "import math\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reader(path, first_var):\n",
    "    '''\n",
    "    Reads SEMS/DASH data, adds datetime columns\n",
    "\n",
    "    :param path: path to data file\n",
    "    :param first_var: the name of the fir column label\n",
    "    :return: pandas DataFrame\n",
    "    '''\n",
    "    # Open the file and read the lines\n",
    "    skip=1\n",
    "    with open(path, \"r\") as file:\n",
    "        # Iterate over the lines\n",
    "        for line in file:\n",
    "            # rip leading and trailing whitespace\n",
    "            line = line.strip()\n",
    "            # Check if the line contains column names\n",
    "            if line.startswith(first_var):\n",
    "                # Split the line by whitespace and append to the columns list\n",
    "                columns = line[1:].strip().split(\"\\t\")\n",
    "                break\n",
    "            skip+=1\n",
    "    # Read the data into a DataFrame, skipping the first 6 rows of comments\n",
    "    d = pd.read_csv(path, sep='\\t', skiprows=skip, names=columns, low_memory=False)\n",
    "    #Check for duplicated metadata, remove metadata rows based on string \"OPC SN\"\n",
    "    if len(d)>0:\n",
    "        if isinstance(d.iloc[0,0], str):\n",
    "            dup_meta = [n for n, i in enumerate(d.iloc[:,0]) if 'OPC SN' in i]\n",
    "            if len(dup_meta) > 0:\n",
    "                for line in dup_meta:\n",
    "                    #Deletes duplicate metadata rows from dataframe\n",
    "                    d.drop(np.arange(line,line+68), inplace = True)\n",
    "                    # Apply the function to each column\n",
    "                    d_og = d.copy()\n",
    "                    for c in d.keys():\n",
    "                        try:\n",
    "                            d[c] = pd.to_numeric(d_og[c])\n",
    "                        except:\n",
    "                            d[c] = d_og[c]\n",
    "                    #d = d.apply(pd.to_numeric, ‘raise’,)\n",
    "                    \n",
    "    # Creates datetime columns\n",
    "    if 'DOY.Frac' in d.keys():\n",
    "        d['dt'] = pd.to_datetime('2024-1-1') + pd.to_timedelta(d['DOY.Frac'], unit='D') - pd.Timedelta(days=1)\n",
    "    if 'StartTimeSt' in d.keys():\n",
    "        d['st_dt'] = pd.to_datetime('2024-1-1') + pd.to_timedelta(d['StartTimeSt'], unit='D') - pd.Timedelta(days=1)\n",
    "    if 'EndTimeSt' in d.keys():\n",
    "        d['end_dt'] = pd.to_datetime('2024-1-1') + pd.to_timedelta(d['EndTimeSt'], unit='D') - pd.Timedelta(days=1)\n",
    "    if 'YY/MM/DD' and 'HR:MN:SC' in d.keys():\n",
    "        d['dt'] = pd.to_datetime(str(20) + d['YY/MM/DD'] + ' ' + d['HR:MN:SC'], format='%Y/%m/%d %H:%M:%S')\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glob_reader(file_key, first_var, subfolder = './data/'):\n",
    "    '''\n",
    "    Reads groups of data files and merges them into one\n",
    "\n",
    "    :param file_key: shared key in filenames\n",
    "    :param first_var: the name of the first column label\n",
    "    :param subfolder: name of the subfolder containing the data\n",
    "    :return: pandas DataFrame\n",
    "    '''\n",
    "    paths = sorted(glob.glob(subfolder+'*'+file_key+'*'))\n",
    "    d = []\n",
    "    for i in range(0, len(paths)):\n",
    "        f = reader(paths[i], first_var)\n",
    "        if len(f)>0:\n",
    "            d.append(f)\n",
    "    d = pd.concat(d).reset_index()\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bin_sum(d):\n",
    "    '''\n",
    "    Adds a column that sums all the bins.\n",
    "\n",
    "    :param d: input pandas DataFrame\n",
    "    :return: pandas DataFrame with 'bin_sum' column\n",
    "    '''\n",
    "    d = d.copy()\n",
    "    col_w_bin = [col for col in d.columns if 'bin' in col]\n",
    "    d['bin_sum'] = d[col_w_bin].sum(numeric_only = True, axis=1)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dNdlogDp(data, bins):\n",
    "    '''\n",
    "    Calculating dN/dlogDp for OPCs. Requires dlogDp to be calculated for bins.\n",
    "\n",
    "    :param data: pandas DataFrame\n",
    "    :param bins: pandas DataFrame of bins\n",
    "    :return: pandas DataFrame\n",
    "    '''\n",
    "    \n",
    "    dNdlogDp = []\n",
    "    for binN in bins.index[:-1]:\n",
    "        dNdlogDp.append(data[f'bin{binN}']/bins.loc[binN, 'dlogDp'])\n",
    "    dNdlogDp = pd.concat(dNdlogDp, axis = 1)\n",
    "    dNdlogDp.columns = [f'{i}_norm' for i in dNdlogDp.columns]\n",
    "    \n",
    "    return dNdlogDp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sems_periods(flight_date):\n",
    "\n",
    "    #Filtering for RF\n",
    "    #This one uses exclusive or (^) but that gives True ^ True = False. Changed to line below it\n",
    "    # line_breaks_RF = line_breaks.loc[(line_breaks['Switch_Start_UTC'].dt.date == flight_date.date()) ^\n",
    "    #                                  (line_breaks['Switch_Stop_UTC'].dt.date == flight_date.date())]\n",
    "    line_breaks_RF = line_breaks.loc[[((line_breaks.loc[i,'Switch_Start_UTC'].date() == flight_date.date()) \n",
    "                                       or (line_breaks.loc[i,'Switch_Stop_UTC'].date() == flight_date.date())) \n",
    "                                      for i in line_breaks.index]]\n",
    "    \n",
    "    #Making sure rows are ordered before we reset index for easier querying\n",
    "    # line_breaks_RF.sort_values('Switch_Start_UTC', inplace = True)\n",
    "    line_breaks_RF.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    #Replace NaT with dummy value based on available timestamp\n",
    "    for i in line_breaks_RF.index:\n",
    "        if type(line_breaks_RF.loc[i, 'Switch_Stop_UTC']) != pd.Timestamp:\n",
    "            line_breaks_RF.loc[i, 'Switch_Stop_UTC'] = line_breaks_RF.loc[i, 'Switch_Start_UTC']+dt.timedelta(seconds = 1)\n",
    "        if type(line_breaks_RF.loc[i, 'Switch_Start_UTC']) != pd.Timestamp:\n",
    "            line_breaks_RF.loc[i, 'Switch_Start_UTC'] = line_breaks_RF.loc[i, 'Switch_Stop_UTC']-dt.timedelta(seconds = 1)\n",
    "        \n",
    "    #Adding rows for first (before first switch) and last (after last switch) timestamps \n",
    "    first_dt = dt.datetime.combine(line_breaks_RF.loc[0, 'Switch_Start_UTC'], time.min) \n",
    "    last_dt = dt.datetime.combine(line_breaks_RF.loc[len(line_breaks_RF)-1, 'Switch_Stop_UTC'], time.max)\n",
    "    line_breaks_RF = line_breaks_RF.copy()\n",
    "    line_breaks_RF.loc[-1, line_breaks_RF.keys()] = [np.nan, np.nan, first_dt, 'OFF', 'SEMS']\n",
    "    line_breaks_RF.loc[len(line_breaks_RF)-1, line_breaks_RF.keys()] = [np.nan, last_dt, np.nan, 'SEMS', 'OFF']\n",
    "    line_breaks_RF.index += 1\n",
    "    line_breaks_RF.sort_index(inplace = True)\n",
    "    # line_breaks_RF.sort_values(by = ['Switch_Start_UTC', 'Switch_Stop_UTC'], inplace = True)\n",
    "    \n",
    "    #Filtering only for SEMS switches\n",
    "    sems_switches = (line_breaks_RF[['LARGE_From', 'LARGE_To']] == 'SEMS').values.sum(1)\n",
    "    line_breaks_RF = line_breaks_RF[sems_switches==1] #These are the switches involving the SEMS\n",
    "    \n",
    "    output = [(math.floor(i/2), \n",
    "               line_breaks_RF.loc[i, f'Switch_{\"Stop\" if i%2==0 else \"Start\"}_UTC']) for i in line_breaks_RF.index]\n",
    "    groups = {}\n",
    "    for l in output:\n",
    "        groups.setdefault(l[0], []).append(l[1])\n",
    "    \n",
    "    output_sorted = list(groups.values())\n",
    "    return output_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flight_periods(flight_date):\n",
    "    out=[]\n",
    "    row =  flight_times.loc[[((flight_times.loc[i,'LARGE_Filter_Off_UTC'].date() == flight_date.date()) \n",
    "                                       or (flight_times.loc[i,'Landing_UTC'].date() == flight_date.date())) \n",
    "                                      for i in flight_times.index]]\n",
    "    return [[pd.to_datetime(row['LARGE_Filter_Off_UTC'].values[0]), pd.to_datetime(row['LARGE_Filter_On_UTC'].values[0])]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(df, sems_per):\n",
    "    combined_mask = pd.Series([False] * len(df))\n",
    "    \n",
    "    for start_date, end_date in sems_per:\n",
    "        start_date = pd.to_datetime(start_date)\n",
    "        end_date = pd.to_datetime(end_date)\n",
    "        mask = (df['dt'] >= start_date) & (df['dt'] <= end_date)\n",
    "        combined_mask = combined_mask | mask  # Combine masks with OR operation\n",
    "    \n",
    "    return df.loc[combined_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(path, f_name, line_sw=False):\n",
    "    min_counts = 2.5\n",
    "    dopc = add_bin_sum(glob_reader('OPC_212', '#YY/MM/DD', subfolder = path))#.dropna(how='any', ignore_index=True)\n",
    "    hopc = add_bin_sum(glob_reader('OPC_210', '#YY/MM/DD', subfolder = path))#.dropna(how='any', ignore_index=True)\n",
    "    dash = glob_reader('DASH_FLOW', '#DOY.Frac', subfolder = path)#.dropna(how='any', ignore_index=True)\n",
    "    sems = glob_reader('SEMS_DATA', '#DOY.Frac', subfolder = path)#.dropna(how='any', ignore_index=True)\n",
    "    \n",
    "    merged = pd.merge_asof(dopc, hopc, on='dt', direction = 'nearest', tolerance=timedelta(seconds=1)).drop(columns=['index_x'])\n",
    "    merged = pd.merge_asof(merged, dash, on='dt', direction = 'nearest', tolerance=timedelta(seconds=1))\n",
    "    merged = pd.merge_asof(merged, sems, on='dt', direction = 'nearest', tolerance=timedelta(seconds=1))\n",
    "    \n",
    "    if line_sw:\n",
    "        merged = crop(merged, sems_per).reset_index().drop(columns=['index'])\n",
    "    merged = crop(merged, ft).reset_index().drop(columns=['index']).dropna(how='any', ignore_index=True)\n",
    "    \n",
    "    merged_diff = merged[['dt', 'UpSt_Dia', 'HM_RH']].diff()\n",
    "    merged[['dt.diff', 'UpSt_Dia.diff', 'HM_RH.diff']] = merged_diff\n",
    "    col_w_bin = [c for c in merged.columns if 'bin' in c and 'sum' not in c]\n",
    "    col_w_DO_bin = [c for c in merged.columns if 'bin' in c and 'x' in c and 'sum' not in c]\n",
    "    col_w_HO_bin = [c for c in merged.columns if 'bin' in c and 'y' in c and 'sum' not in c]\n",
    "    new_group = True\n",
    "    start_i = []\n",
    "    end_i = []\n",
    "    delay = False\n",
    "    for i in range(0,len(merged)):\n",
    "        row = merged.iloc[i]\n",
    "        if delay == False:\n",
    "            if new_group:\n",
    "                start_i.append(i)\n",
    "                new_group = False\n",
    "            if abs(row['UpSt_Dia.diff']) > 0:\n",
    "                end_i.append(i)\n",
    "                new_group = True\n",
    "                dt_switch = row['dt']+timedelta(seconds=16)\n",
    "                delay = True\n",
    "                c_i = 0\n",
    "            elif abs(row['HM_RH.diff']) > 5:\n",
    "                end_i.append(i)\n",
    "                new_group = True\n",
    "        \n",
    "        else:\n",
    "            if row['dt'] > dt_switch:\n",
    "                delay = False\n",
    "        if i == len(merged)-1:\n",
    "            if len(start_i) > len(end_i):\n",
    "                end_i.append(i)\n",
    "    \n",
    "    '''\n",
    "    for i in range(0, len(start_i)):\n",
    "        if i < len(start_i)-1:\n",
    "            print(end_i[i]-start_i[i])\n",
    "    '''\n",
    "    \n",
    "    col = ['Samp_Num', 'Start_Date_Time_UTC', 'Mid_Date_Time_UTC', 'Stop_Date_Time_UTC', 'Samp_Time_s', 'data_points', 'UpSt_Dia', 'UpSt_Dia_Sdev', 'HM_RH_Av', 'HM_RH_Sdev', 'HO_RH_Av', 'HO_RH_Sdev', 'HO_below_cnts', 'min_DO_sample_flw', 'min_HO_sample_flw']\n",
    "    comb_bins = []\n",
    "    dopc_labels = []\n",
    "    hopc_labels = []\n",
    "    for i in range(1,73):\n",
    "        if i < 10:\n",
    "            dopc_labels.append('DO_Bin0'+str(i))\n",
    "            hopc_labels.append('HO_Bin0'+str(i))\n",
    "        else:\n",
    "            dopc_labels.append('DO_Bin'+str(i))\n",
    "            hopc_labels.append('HO_Bin'+str(i))\n",
    "        \n",
    "        if i < 10:\n",
    "            col.append('DO_Bin0'+str(i))\n",
    "            comb_bins.append('DO_Bin0'+str(i))\n",
    "        else:\n",
    "            col.append('DO_Bin'+str(i))\n",
    "            comb_bins.append('DO_Bin'+str(i))\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(1,73):\n",
    "        if i < 10:\n",
    "            col.append('HO_Bin0'+str(i))\n",
    "            comb_bins.append('HO_Bin0'+str(i))\n",
    "        else:\n",
    "            col.append('HO_Bin'+str(i))\n",
    "            comb_bins.append('HO_Bin'+str(i))\n",
    "    output = pd.DataFrame(columns = col)\n",
    "    out_acc = 0\n",
    "    data_points = 0\n",
    "    for i in tqdm(range(0,len(start_i))):\n",
    "        subset = merged.iloc[start_i[i]:end_i[i]+1].reset_index()\n",
    "        DO_n_sum = 0\n",
    "        HO_n_sum = 0\n",
    "        s = 0\n",
    "        place = False\n",
    "        for j in range(0,len(subset)):\n",
    "            dt_start = subset.loc[s,'dt']\n",
    "            DO_n_sum = subset.loc[s:j+1, col_w_DO_bin].sum().max()\n",
    "            HO_n_sum = subset.loc[s:j+1, col_w_HO_bin].sum().max()\n",
    "            if subset.loc[j, 'dt.diff'] > timedelta(seconds=2):\n",
    "                s=j+1\n",
    "                continue\n",
    "            elif subset.loc[j, 'dt'] - dt_start > timedelta(minutes=5):\n",
    "                s=j+1\n",
    "                continue\n",
    "            \n",
    "            #print((bins['Dp_nm'] - float(subset.loc[j, 'UpSt_Dia'])).abs().idxmin())\n",
    "            #print(float(subset.loc[j, 'UpSt_Dia']))\n",
    "            \n",
    "            if DO_n_sum > min_counts / float((bins.loc[[(bins['Dp_nm'] - float(subset.loc[j, 'UpSt_Dia'])).abs().idxmin()], 'dlogDp']).iloc[0]):\n",
    "                #print(len(output)-1, out_acc, len(subset)-1, s)\n",
    "                output.loc[out_acc, 'Start_Date_Time_UTC'] = subset.loc[s, 'dt']\n",
    "                output.loc[out_acc, 'Stop_Date_Time_UTC'] = subset.loc[j, 'dt']\n",
    "                output.loc[out_acc, 'Mid_Date_Time_UTC'] = subset.loc[s, 'dt']+(subset.loc[j, 'dt']-subset.loc[s, 'dt'])/2\n",
    "                output.loc[out_acc, 'Samp_Time_s'] = (subset.loc[j, 'dt']-subset.loc[s, 'dt']+timedelta(seconds=1)).total_seconds()\n",
    "\n",
    "                output.loc[out_acc, 'min_DO_sample_flw'] = subset.loc[s:j+1, 'sample_flw_x'].min()\n",
    "                output.loc[out_acc, 'min_HO_sample_flw'] = subset.loc[s:j+1, 'sample_flw_y'].min()\n",
    "                \n",
    "                output.loc[out_acc, 'data_points'] = j-s\n",
    "                data_points += output.loc[out_acc, 'data_points']\n",
    "\n",
    "                output.loc[out_acc, 'UpSt_Dia'] = subset.loc[s:j+1, 'UpSt_Dia'].mean()\n",
    "                output.loc[out_acc, 'HM_RH_Av'] = subset.loc[s:j+1, 'HM_RH'].mean()\n",
    "                output.loc[out_acc, 'HO_RH_Av'] = subset.loc[s:j+1, 'HO_RH'].mean()\n",
    "\n",
    "                output.loc[out_acc, 'UpSt_Dia_Sdev'] = subset.loc[s:j+1, 'UpSt_Dia'].std()\n",
    "                output.loc[out_acc, 'HM_RH_Sdev'] = subset.loc[s:j+1, 'HM_RH'].std()\n",
    "                output.loc[out_acc, 'HO_RH_Sdev'] = subset.loc[s:j+1, 'HO_RH'].std()\n",
    "\n",
    "                output.loc[out_acc, comb_bins] = list(subset.loc[s:j+1, col_w_bin].sum())\n",
    "                if HO_n_sum*2 > min_counts / float((bins.loc[[(bins['Dp_nm'] - float(subset.loc[j, 'UpSt_Dia'])).abs().idxmin()], 'dlogDp']).iloc[0]):\n",
    "                    output.loc[out_acc, 'HO_below_cnts'] = 0\n",
    "                else:\n",
    "                    output.loc[out_acc, 'HO_below_cnts'] = 1\n",
    "                s = j+1\n",
    "    \n",
    "                HO_n_sum = 0\n",
    "                DO_n_sum = 0\n",
    "                out_acc += 1\n",
    "\n",
    "    output = output[output['HM_RH_Sdev']<5]\n",
    "    output = output[output['UpSt_Dia_Sdev']==0]\n",
    "    output = output.drop(columns=['UpSt_Dia_Sdev'])\n",
    "\n",
    "    d_m_b = output[dopc_labels].astype('float').idxmax(axis=1)\n",
    "    output = output[(d_m_b != 'DO_Bin01') & (d_m_b != 'DO_Bin02')]\n",
    "\n",
    "    output = output[output['min_DO_sample_flw'] >= 0]\n",
    "    output.loc[output['min_HO_sample_flw'] < 0, 'HO_below_cnts'] = 1\n",
    "\n",
    "    h_m_b = output[hopc_labels].astype('float').idxmax(axis=1)\n",
    "\n",
    "    output.loc[(h_m_b != 'HO_Bin01') & (h_m_b != 'HO_Bin02'), 'HO_below_cnts'] = 0\n",
    "\n",
    "    output['Samp_Num'] = output.reset_index().index+1\n",
    "    output = output.reset_index(drop=True)\n",
    "    output.to_csv('./processed_opc/DASH_SAMP_PARAM_'+f_name+'.csv', index=None)\n",
    "\n",
    "    if line_sw:\n",
    "        dopc_c = crop(dopc, ft).reset_index().drop(columns=['index'])\n",
    "        dopc_c = crop(dopc_c, sems_per).reset_index().drop(columns=['index'])\n",
    "    else:\n",
    "        dopc_c = crop(dopc, ft).reset_index().drop(columns=['index'])\n",
    "\n",
    "    print('Fraction of data used =', str(round(output['data_points'].sum()/len(dopc_c), 2)))\n",
    "    print('Data with usable GF =', str(len(output[output['HO_below_cnts']==0])/len(output)))\n",
    "\n",
    "    s_t = sorted(list(output['Start_Date_Time_UTC'])+list(output['Stop_Date_Time_UTC']))\n",
    "    fig, axes = plt.subplots(2, figsize=(5, 2.5), sharex='all')\n",
    "\n",
    "    ax=axes[0]\n",
    "\n",
    "    output['Time_Diff'] = output['Mid_Date_Time_UTC'].diff().dt.total_seconds() / 60\n",
    "\n",
    "    # Identify the indices where the time difference is greater than 5 minutes\n",
    "    gap_indices = output.index[output['Time_Diff'] > 5].tolist()\n",
    "\n",
    "    old_i = 0\n",
    "    output_GF = output[output['HO_below_cnts']==0]\n",
    "\n",
    "    c = ['black', 'red']\n",
    "    for i in gap_indices:\n",
    "        ax.plot(output.loc[old_i:i-1,'Mid_Date_Time_UTC'], output.loc[old_i:i-1,'Samp_Time_s'], c='black')\n",
    "        #ax.scatter(output_GF.loc[old_i:i-1,'Mid_Date_Time_UTC'], output_GF.loc[old_i:i-1,'Samp_Time_s'], c='lime', s=1, zorder=1000)\n",
    "        ax.set_ylabel('Samp\\nDuration [s]\\n', rotation=0, labelpad=40, loc='bottom')\n",
    "        old_i = i\n",
    "        \n",
    "    ax.plot(output.loc[old_i:, 'Mid_Date_Time_UTC'], output.loc[old_i:, 'Samp_Time_s'], c='black')\n",
    "\n",
    "    ax=axes[1]\n",
    "    ax.scatter(dopc['dt'], ['Raw']*len(dopc), s=.1)\n",
    "    ax.scatter(s_t, ['Retrieved']*len(s_t), s=.1)\n",
    "    ax.set_ylim(-1,3)\n",
    "    date_format = mdates.DateFormatter('%H')\n",
    "    ax.xaxis.set_major_formatter(date_format)\n",
    "    ax.xaxis.set_major_locator(mdates.HourLocator(interval=1))\n",
    "    ax.set_xlabel('Hour [UTC]')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig('./figures/process-'+f_name+'.png')\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = pd.read_csv('./data/DASH_Bins_2023.csv').set_index('BinNum')\n",
    "meta_dir = './meta/'\n",
    "flight_times = pd.read_csv(f'{meta_dir}ARCSIX_takeoff_landing_times.txt',\n",
    "                           parse_dates = ['Takeoff_UTC', 'Landing_UTC', 'LARGE_Filter_Off_UTC', 'LARGE_Filter_On_UTC']\n",
    "                           ).set_index('FltNum')\n",
    "line_breaks = pd.read_csv(f'{meta_dir}ARCSIX_DASH_SEMS_switch_times.txt',\n",
    "                            parse_dates = ['Switch_Start_UTC', 'Switch_Stop_UTC']\n",
    "                            )[['FltNum', 'Switch_Start_UTC', 'Switch_Stop_UTC', \n",
    "                               'LARGE_From', 'LARGE_To']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/128 [02:42<5:44:21, 162.69s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[365], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#sems_per = get_sems_periods(datetime.strptime(date, '%Y_%m_%d'))\u001b[39;00m\n\u001b[1;32m      3\u001b[0m ft \u001b[38;5;241m=\u001b[39m get_flight_periods(datetime\u001b[38;5;241m.\u001b[39mstrptime(date, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m----> 4\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data/DASH-flight-\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mdate\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRESAMP_240605\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[362], line 95\u001b[0m, in \u001b[0;36mprocess\u001b[0;34m(path, f_name, line_sw)\u001b[0m\n\u001b[1;32m     93\u001b[0m DO_n_sum \u001b[38;5;241m=\u001b[39m subset\u001b[38;5;241m.\u001b[39mloc[s:j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, col_w_DO_bin]\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mmax()\n\u001b[1;32m     94\u001b[0m HO_n_sum \u001b[38;5;241m=\u001b[39m subset\u001b[38;5;241m.\u001b[39mloc[s:j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, col_w_HO_bin]\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mmax()\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43msubset\u001b[49m\u001b[38;5;241m.\u001b[39mloc[j, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdt.diff\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m timedelta(seconds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m     96\u001b[0m     s\u001b[38;5;241m=\u001b[39mj\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[362], line 95\u001b[0m, in \u001b[0;36mprocess\u001b[0;34m(path, f_name, line_sw)\u001b[0m\n\u001b[1;32m     93\u001b[0m DO_n_sum \u001b[38;5;241m=\u001b[39m subset\u001b[38;5;241m.\u001b[39mloc[s:j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, col_w_DO_bin]\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mmax()\n\u001b[1;32m     94\u001b[0m HO_n_sum \u001b[38;5;241m=\u001b[39m subset\u001b[38;5;241m.\u001b[39mloc[s:j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, col_w_HO_bin]\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mmax()\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43msubset\u001b[49m\u001b[38;5;241m.\u001b[39mloc[j, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdt.diff\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m timedelta(seconds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m     96\u001b[0m     s\u001b[38;5;241m=\u001b[39mj\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_trace_dispatch_regular.py:204\u001b[0m, in \u001b[0;36mtrace_dispatch\u001b[0;34m(py_db, frame, event, arg)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m apply_to_settrace:\n\u001b[1;32m    203\u001b[0m     py_db\u001b[38;5;241m.\u001b[39menable_tracing(thread_trace_func)\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mthread_trace_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_trace_dispatch_regular.py:440\u001b[0m, in \u001b[0;36mThreadTracer.__call__\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m NO_FTRACE\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# if DEBUG: print('trace_dispatch', filename, frame.f_lineno, event, frame.f_code.co_name, file_type)\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# Just create PyDBFrame directly (removed support for Python versions < 2.5, which required keeping a weak\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;66;03m# reference to the frame).\u001b[39;00m\n\u001b[1;32m    436\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mPyDBFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpy_db\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mabs_path_canonical_path_and_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madditional_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_skips_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m--> 440\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;66;03m# 1 means skipped because of filters.\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;66;03m# 2 means skipped because no breakpoints were hit.\u001b[39;00m\n\u001b[1;32m    444\u001b[0m     cache_skips[frame_cache_key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py:1197\u001b[0m, in \u001b[0;36mPyDBFrame.trace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_line:\n\u001b[1;32m   1196\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_suspend(thread, step_cmd, original_step_cmd\u001b[38;5;241m=\u001b[39minfo\u001b[38;5;241m.\u001b[39mpydev_original_step_cmd)\n\u001b[0;32m-> 1197\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_return:  \u001b[38;5;66;03m# return event\u001b[39;00m\n\u001b[1;32m   1199\u001b[0m     back \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mf_back\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py:165\u001b[0m, in \u001b[0;36mPyDBFrame.do_wait_suspend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_wait_suspend\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "date = '2024_06_05'\n",
    "#sems_per = get_sems_periods(datetime.strptime(date, '%Y_%m_%d'))\n",
    "ft = get_flight_periods(datetime.strptime(date, '%Y_%m_%d'))\n",
    "output = process('./data/DASH-flight-'+date+'/', 'RESAMP_240605', )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
